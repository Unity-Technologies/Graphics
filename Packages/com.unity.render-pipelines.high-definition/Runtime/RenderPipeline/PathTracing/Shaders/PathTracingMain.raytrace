// We do not rely on recursion, beyond shooting shadow and random walk rays from the intersected surface
#pragma max_recursion_depth 2

// SRP/HDRP includes
#define SHADER_TARGET 50
// Include and define the shader pass (Required for light cluster code)
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/ShaderPass/ShaderPass.cs.hlsl"
#define SHADERPASS SHADERPASS_PATH_TRACING

#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Sampling/Sampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/ShaderLibrary/ShaderVariables.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/Builtin/BuiltinData.hlsl"

// We need this for the potential volumetric integration on camera misses
#define HAS_LIGHTLOOP

// Ray tracing includes
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/ShaderVariablesRaytracing.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/Common/AtmosphericScatteringRayTracing.hlsl"

// Path tracing includes
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingPayload.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingSampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingSkySampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingVolume.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/PathTracing/Shaders/PathTracingAOV.hlsl"

// Input(s)
float4x4    _PixelCoordToViewDirWS;
int         _PathTracingCameraSkyEnabled;
float4      _PathTracingCameraClearColor;
float4      _PathTracingDoFParameters;    // x: aperture radius, y: focus distance, zw: unused
float4      _PathTracingTilingParameters; // xy: tile count, zw: current tile index

// Output(s)
RW_TEXTURE2D_X(float4, _FrameTexture);

// AOVs
RW_TEXTURE2D_X(float4, _AlbedoAOV);
RW_TEXTURE2D_X(float4, _NormalAOV);
RW_TEXTURE2D_X(float4, _MotionVectorAOV);

[shader("miss")]
void CameraMiss(inout PathPayload payload : SV_RayPayload)
{
    // Set initial "hit" to infinity (before potential volumetric scattering) and alpha to the proper value
    payload.rayTHit = FLT_INF;
    payload.alpha = IsSkyEnabled() && _PathTracingCameraSkyEnabled ? 1.0 : min(_PathTracingCameraClearColor.a, 1.0);

    // In indirect-only mode, it makes more sense to return a null value
    if (_RaytracingMinRecursion > 1)
    {
        payload.value = 0.0;
        return;
    }

    // We use the pixel coordinates to access the skytexture, so we first have to remove the tiling 
    uint2 tileCount = uint2(_PathTracingTilingParameters.xy);
    uint2 pixelCoord = payload.pixelCoord / tileCount; 
    payload.value = IsSkyEnabled() && _PathTracingCameraSkyEnabled ?
        GetSkyBackground(pixelCoord).rgb : _PathTracingCameraClearColor.rgb * GetInverseCurrentExposureMultiplier();

    ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.value, payload.alpha);

    if (_EnableVolumetricFog)
    {
        float3 lightPosition, envValue = payload.value;

        // Generate a 4D unit-square sample for this depth, from our QMC sequence
        float4 inputSample = GetSample4D(payload.pixelCoord, _RaytracingSampleIndex, 0);

        // Compute volumetric scattering
        payload.value = 0.0;
        float volPdf = 1.0;
        bool sampleLocalLights;
        if (SampleVolumeScatteringPosition(payload.pixelCoord, inputSample.w, payload.rayTHit, volPdf, sampleLocalLights, lightPosition))
        {
            ComputeVolumeScattering(payload, inputSample.xyz, sampleLocalLights, lightPosition);

            // Apply volumetric attenuation
            ApplyFogAttenuation(WorldRayOrigin(), WorldRayDirection(), payload.rayTHit, payload.value, false);

            // Apply the volumetric PDF
            payload.value /= volPdf;
        }

        // Reinject the environment value
        payload.value += envValue;
    }

    // Override AOV motion vector information
    payload.aovMotionVector = 0.0;
}

[shader("miss")]
void EmptyMiss(inout PathPayload payload : SV_RayPayload)
{
}

void ApplyDepthOfField(uint2 pixelCoord, float dotDirection, inout float3 origin, inout float3 direction)
{
     // Check aperture radius
    if (_PathTracingDoFParameters.x <= 0.0)
        return;

    // Sample the lens aperture using the next available dimensions
    // (we use 40 for path tracing, 2 for sub-pixel jittering, 64 for SSS -> 106, 107)
    float2 uv = _PathTracingDoFParameters.x * SampleDiskUniform(GetSample(pixelCoord, _RaytracingSampleIndex, 106),
                                                                GetSample(pixelCoord, _RaytracingSampleIndex, 107));

    // Compute the focus point by intersecting the pinhole ray with the focus plane
    float t = _PathTracingDoFParameters.y / dotDirection;
    float3 focusPoint = origin + t * direction;

    // Compute the new ray origin (_ViewMatrix[0] = right, _ViewMatrix[1] = up)
    origin += _ViewMatrix[0].xyz * uv.x + _ViewMatrix[1].xyz * uv.y;

    // The new ray direction should pass through the focus point
    direction = normalize(focusPoint - origin);
}

void GenerateCameraRay(uint2 pixelCoord, out PathPayload payload, out RayDesc ray, bool withAOV = false)
{
    // Get the current tile coordinates (for interleaved tiling) and update pixel coordinates accordingly
    uint2 tileCount = uint2(_PathTracingTilingParameters.xy);
    uint2 tileIndex = uint2(_PathTracingTilingParameters.zw);
    uint2 tiledPixelCoord = pixelCoord * tileCount + tileIndex;

    // Jitter them (we use 4x10 dimensions of our sequence during path tracing atm, so pick the next available ones)
    float4 jitteredPixelCoord = float4(pixelCoord, 1.0, 1.0);
    jitteredPixelCoord.x += GetSample(tiledPixelCoord, _RaytracingSampleIndex, 40) / tileCount.x;
    jitteredPixelCoord.y += GetSample(tiledPixelCoord, _RaytracingSampleIndex, 41) / tileCount.y;

    // Initialize the payload for this camera ray
    payload.throughput = 1.0;
    payload.maxRoughness = 0.0;
    payload.rayDirection = 0.0;
    payload.pixelCoord = tiledPixelCoord;
    payload.segmentID = 0;

    // In order to achieve texture filtering, we need to compute the spread angle of the subpixel
    payload.cone.spreadAngle = _RaytracingPixelSpreadAngle / min(tileCount.x, tileCount.y);
    payload.cone.width = 0.0;

    // The motion vector AOV value will also serve as a on/off indicator for AOV computations
    payload.aovMotionVector = withAOV ? jitteredPixelCoord.xy : 0.0;
    payload.aovAlbedo = 0.0;
    payload.aovNormal = 0.0;

    // Generate the ray descriptor for this pixel
    ray.TMin = _RaytracingCameraNearPlane;
    ray.TMax = FLT_INF;

    // We need the camera forward direction in both types of projection
    float3 cameraDirection = GetViewForwardDir();

    // Compute the ray's origin and direction, for either perspective or orthographic projection
    if (IsPerspectiveProjection())
    {
        ray.Origin = GetPrimaryCameraPosition();
        ray.Direction = -normalize(mul(jitteredPixelCoord, _PixelCoordToViewDirWS).xyz);

        // Use planar clipping, to match rasterization
        float dotDirection = dot(cameraDirection, ray.Direction);
        ray.TMin /= dotDirection;

        ApplyDepthOfField(tiledPixelCoord, dotDirection, ray.Origin, ray.Direction);
    }
    else // Orthographic projection
    {
        uint2 pixelResolution = DispatchRaysDimensions().xy;
        float4 screenCoord = float4(2.0 * jitteredPixelCoord.x / pixelResolution.x - 1.0,
                                    -2.0 * jitteredPixelCoord.y / pixelResolution.y + 1.0,
                                    0.0, 0.0);

        ray.Origin = mul(_InvViewProjMatrix, screenCoord).xyz;
        ray.Direction = cameraDirection;
    }
}

float3 ClampValue(float3 value, float maxIntensity = _RaytracingIntensityClamp)
{
    float intensity = Luminance(value) * GetCurrentExposureMultiplier();
    return intensity > maxIntensity ? value * maxIntensity / intensity : value;
}

void TracePath(uint2 pixelCoord, bool withAOV = false)
{
    // Generate the camera segment of our path
    PathPayload payload;
    RayDesc ray;
    GenerateCameraRay(pixelCoord, payload, ray, withAOV);

    // Trace our camera ray
    TraceRay(_RaytracingAccelerationStructure, RAY_FLAG_CULL_BACK_FACING_TRIANGLES, RAYTRACINGRENDERERFLAG_PATH_TRACING, 0, 1, 0, ray, payload);

    // These are the quantities we want to compute for the path
    float3 value = payload.value;
    float alpha = payload.alpha;

    // Test if something was hit from our camera ray
    if (payload.rayTHit < FLT_INF)
    {
        // Iterate over each path segments, up to a specified max recursion depth
        for (uint segmentID = 1; segmentID < _RaytracingMaxRecursion && any(payload.rayDirection); segmentID++)
        {
            // Store the throughput up to that segment
            float3 throughput = payload.throughput;

            // Update our continuation ray
            GetContinuationRay(payload, ray);

            // Reset relevant fields of the payload
            payload.value = 0.0;
            payload.alpha = 0.0;
            payload.rayDirection = 0.0;
            payload.segmentID = segmentID;

            // Trace our continuation ray
            TraceRay(_RaytracingAccelerationStructure, RAY_FLAG_CULL_BACK_FACING_TRIANGLES,
                     RAYTRACINGRENDERERFLAG_PATH_TRACING, 0, 1, 1, ray, payload);

            // Add the segment contribution to our overall radiance and alpha values
            value += throughput * ClampValue(payload.value);
            alpha += (1.0 - alpha) * payload.alpha;
        }
    }

    // Copy the full path radiance value (and alpha/presence) to our output buffer
    _FrameTexture[COORD_TEXTURE2D_X(pixelCoord)] = float4(value, alpha);

    if (withAOV)
    {
        // If we computed AOVs, copy relevant values to our output buffers
        _AlbedoAOV[COORD_TEXTURE2D_X(pixelCoord)] = float4(payload.aovAlbedo, 1.0);
        _NormalAOV[COORD_TEXTURE2D_X(pixelCoord)] = float4(payload.aovNormal, 1.0);
        _MotionVectorAOV[COORD_TEXTURE2D_X(pixelCoord)] = float4(payload.aovMotionVector, 0.0, 1.0);
    }
}

[shader("raygeneration")]
void RayGen()
{
    TracePath(DispatchRaysIndex().xy);
}

[shader("raygeneration")]
void RayGenAOV()
{
    TracePath(DispatchRaysIndex().xy, true);
}
