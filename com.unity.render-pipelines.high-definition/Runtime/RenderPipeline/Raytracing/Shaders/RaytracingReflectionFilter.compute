#pragma kernel RaytracingReflectionFilter
#pragma kernel TemporalAccumulationFilter
#pragma kernel RaytracingReflectionTAA

#pragma kernel ReflBilateralFilterH   ReflBilateralFilter=ReflBilateralFilterH
#pragma kernel ReflBilateralFilterV   ReflBilateralFilter=ReflBilateralFilterV  FINAL_PASS

#pragma only_renderers d3d11
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/ShaderLibrary/ShaderVariables.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/NormalBuffer.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/Builtin/BuiltinData.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/BSDF.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/PreIntegratedFGD/PreIntegratedFGD.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/PostProcessing/Shaders/TemporalAntialiasing.hlsl"

// Raytracing Includes
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/OnlineVariance.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/RaytracingConsts.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/RaytracingSampling.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/Raytracing/Shaders/ShaderVariablesRaytracing.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Lighting/ScreenSpaceLighting/ScreenSpaceLighting.hlsl"

// Tile size of this compute
#define RAYTRACING_REFLECTION_TILE_SIZE 8

// Input textures for the spatial filtering
Texture2D<float>                    _DepthTexture;
Texture2DArray<float>               _NoiseTexture;
Texture2D<float4>                   _SsrLightingTextureRW;
Texture2D<float4>                   _SsrHitPointTexture;
Texture2D<float4>                   _SsrClearCoatMaskTexture;

// Output Textures for the spatial filtering
RWTexture2D<float4>                 _RaytracingReflectionTexture;
RWTexture2D<float>                  _VarianceTexture;
RWTexture2D<float3>                 _MaxColorRangeTexture;
RWTexture2D<float3>                 _MinColorRangeTexture;
int                                 _SpatialFilterRadius;

// Input and Output data of the temporal accumulation pass
RWTexture2D<float4>                 _CurrentFrameTexture;
RWTexture2D<float4>                 _AccumulatedFrameTexture;
float                               _TemporalAccumuationWeight;

// Buffers for the temporal filtering
Texture2D<float4>           _ReflectionHistorybufferRW;
Texture2D<float4>           _DenoiseInputTexture;
RWTexture2D<float4>         _DenoiseOutputTextureRW;
int                         _RaytracingDenoiseRadius;

[numthreads(RAYTRACING_REFLECTION_TILE_SIZE, RAYTRACING_REFLECTION_TILE_SIZE, 1)]
void RaytracingReflectionFilter(uint3 dispatchThreadId : SV_DispatchThreadID, uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{

    uint2 fullResCoord = dispatchThreadId.xy;
    uint2 halfResCoord = fullResCoord / 2;

    // Compute the index of the noise texture to use
    int noiseIndex = (int)(clamp((int)(_ScramblingTexture[halfResCoord].y * 32.0f), 0, 31));

    // Compute the subpixel index that matches this full screen pixel.
    int localIndex = (fullResCoord.x & 1) + (fullResCoord.y & 1) * 2;

    // Fetch the depth
    float depth = LOAD_TEXTURE2D(_DepthTexture, fullResCoord).x;

    NormalData normalData;
    DecodeFromNormalBuffer(fullResCoord, normalData);

    // We use a texture to identify if we use a clear coat constant for perceptualRoughness for SSR or use value from normal buffer.
    // When we use a forward material we can output the normal and perceptualRoughness for the coat for SSR, so we simply bind a black 1x1 texture
    // When we use deferred material we need to bind the gbuffer2 and read the coat mask
    float4 coatMask = _SsrClearCoatMaskTexture[fullResCoord];
    normalData.perceptualRoughness = HasClearCoatMask(coatMask) ? CLEAR_COAT_PERCEPTUAL_ROUGHNESS : normalData.perceptualRoughness;
    // Fetch the roughness
    float roughness = PerceptualRoughnessToRoughness(normalData.perceptualRoughness);

    // Duplicating same early out condition we do on reflection dispatchrays as that info is 1/2 res while we need full res granularity here.
    // Also, this operates on data we fetch anyway, while the _SsrLightingTextureRW at central pixel is needed only if that pixel contributes to filtering below. 
    if (depth == UNITY_RAW_FAR_CLIP_VALUE || PerceptualRoughnessToPerceptualSmoothness(normalData.perceptualRoughness) < _RaytracingReflectionMinSmoothness)
        return;

    // Fetch the normal WS
    float3 normalWS = normalData.normalWS;

    // Compute the world space position
    PositionInputs posInput = GetPositionInput_Stereo(fullResCoord, _ScreenSize.zw, depth, UNITY_MATRIX_I_VP, UNITY_MATRIX_V, unity_StereoEyeIndex);
    float3 positionWS = GetAbsolutePositionWS(posInput.positionWS);

    // Compute the view in world space
    float3 viewWS = normalize(_WorldSpaceCameraPos - positionWS);

    // Compute the reflected direction for this view direction
    float3 reflDir = reflect(-viewWS, normalWS);

    // Initialize the output pixels
    float4 resultSum = float4(0.0 ,0.0, 0.0, 0.0);
    float3 minColorRange = float3(1000.0, 1000.0, 1000.0);
    float3 maxColorRange = float3(0.0, 0.0, 0.0);
    uint sampleCount = 0;

    VarianceEstimator variance;
    InitializeVarianceEstimator(variance);

    float radiusSq = _SpatialFilterRadius * _SpatialFilterRadius;
    
    for(int y = -_SpatialFilterRadius; y < _SpatialFilterRadius; ++y)
    {
        for(int x = -_SpatialFilterRadius; x < _SpatialFilterRadius; ++x)
        {
            float radiusDistanceSq = (y*y + x*x);
            if(radiusDistanceSq > radiusSq) continue;

            // Compute the noise position that shall be used
            int2 relativeHRShift = uint2(8 + x, 8 + y);

            // Full res sample position
            int2 sourceCoord = (halfResCoord + uint2(x,y)) * 2;

            // If this pixel is outside of the screen, we cannot use it
            if(sourceCoord.x < 0 || sourceCoord.x > _ScreenSize.x 
                || sourceCoord.y < 0 || sourceCoord.y > _ScreenSize.y) 
            continue;
            
            // Fetch the target color
            float4 sampleColor = _SsrLightingTextureRW[sourceCoord];

            // Compute the position of the actual source pixel
            uint subPixel =  clamp(floor(sampleColor.w * 4.0f), 0, 3);
            uint2 shift = HalfResIndexToCoordinateShift[subPixel];
            uint2 actualSourceCoord = sourceCoord + shift;

            // Fetch the Depth
            float sampleDepth = LOAD_TEXTURE2D(_DepthTexture, actualSourceCoord).x;
            // If this the background, it should not be used as a valid sample
            if(sampleDepth == 0.0f) continue;

            // Compute the target pixel that it will impact
            float sample = _NoiseTexture[int3(relativeHRShift, noiseIndex)].x;
            int index = clamp(floor(sample * 4.0f), 0, 3);

            if (index != localIndex) continue;

            // Let's fetch the half res sample's properties
            // Get the direction and pdf
            float4 directionPDF = _SsrHitPointTexture[sourceCoord];

            // If this direction is under the candidate surface, then it is not valid
            if(dot(directionPDF.xyz, normalWS) <= 0.0f) continue;

            // If this direction is not in the hemisphere of the reflected view direction, then it is not valid
            if(dot(directionPDF.xyz, reflDir) <= 0.0f) continue;

            // Compute the brdf of this sample
            float weight = 1.0f;
            if(roughness > 0.001)
            {
                // Compute the brdf of this sample
                float3 H = normalize(directionPDF.xyz + viewWS);
                float NdotH = dot(normalWS, H);
                float NdotL = dot(directionPDF.xyz, normalWS);
                float NdotV = dot(viewWS, normalWS);
                float localBRDF = D_GGX(NdotH, roughness) * V_SmithJointGGX(NdotL, NdotV, roughness) * NdotL;
                weight = localBRDF * directionPDF.w;
            }

            // Push the value to the variance estimation
            PushValue(variance, length(sampleColor.xyz));

            // Contirbute to all the output values
            float3 sampleResult = sampleColor.xyz * weight;
            resultSum += float4(sampleResult, weight);
            minColorRange = min(minColorRange[index], sampleResult);
            maxColorRange = max(maxColorRange[index], sampleResult);
            sampleCount += 1;
        }
    }

        // Compute the full res coordinate
        if(depth == 0.0f || sampleCount == 0)
        {
            _RaytracingReflectionTexture[fullResCoord] = float4(0.0f, 0.0f, 0.0f, 0.0f);
            _VarianceTexture[fullResCoord] = 1.0f;
            _MaxColorRangeTexture[fullResCoord] = float3(0.0, 0.0, 0.0);
            _MinColorRangeTexture[fullResCoord] = float3(1.0, 1.0, 1.0);
        }
        else
        {
            _RaytracingReflectionTexture[fullResCoord] = float4((resultSum.xyz / resultSum.w), roughness);
            _VarianceTexture[fullResCoord] = saturate(Variance(variance));
            _MaxColorRangeTexture[fullResCoord] = maxColorRange;
            _MinColorRangeTexture[fullResCoord] = minColorRange;
        }
}

[numthreads(RAYTRACING_REFLECTION_TILE_SIZE, RAYTRACING_REFLECTION_TILE_SIZE, 1)]
void TemporalAccumulationFilter(uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    // Fetch the current pixel coordinate
    uint2 currentCoord = groupId * RAYTRACING_REFLECTION_TILE_SIZE + groupThreadId;
    currentCoord.x = currentCoord.x + (unity_StereoEyeIndex * _ScreenSize.x);

    // Fetch the previous color
    float3 previousColor = _AccumulatedFrameTexture[currentCoord].xyz;
    bool previousValidityFlag = _AccumulatedFrameTexture[currentCoord].w > 0.0f;

    // Fetch the color range that we need to check before using
    float3 colorMinBound = _MinColorRangeTexture[currentCoord].xyz;
    float3 colorMaxBound = _MaxColorRangeTexture[currentCoord].xyz;

    // check if the previous color is in the bounds
    // TODO: Try to do the comparison in Lab for better results http://www.brucelindbloom.com/index.html?Math.html
    bool colorInBound = colorMinBound.x < previousColor.x && colorMaxBound.x > previousColor.x 
                        && colorMinBound.y < previousColor.y && colorMaxBound.y > previousColor.y 
                        && colorMinBound.z < previousColor.z && colorMaxBound.z > previousColor.z;

    // Validity flag of the current sample
    float validityFlag = all(colorMinBound < colorMaxBound);
    
    float3 combinedColor = float3(0.0f, 0.0f, 0.0f);
    if(previousValidityFlag && colorInBound)
    {
        // Compute the accumulation factor for this surface (using the user parameter and the rouhgness of the surface)
        float accumulationFactor = _CurrentFrameTexture[currentCoord].w < 0.001f ? 1.0 : _TemporalAccumuationWeight;  

        // Previous pixel is valid
        combinedColor = (_CurrentFrameTexture[currentCoord].xyz * accumulationFactor + _AccumulatedFrameTexture[currentCoord].xyz * (1.0 - accumulationFactor));
    }
    else
    {
        // Previous pixel is invalid, override it
        combinedColor = _CurrentFrameTexture[currentCoord].xyz;
    }
    
    _AccumulatedFrameTexture[currentCoord] = float4(combinedColor, validityFlag);
    _CurrentFrameTexture[currentCoord] = float4(combinedColor, 1.0);
}

[numthreads(RAYTRACING_REFLECTION_TILE_SIZE, RAYTRACING_REFLECTION_TILE_SIZE, 1)]
void RaytracingReflectionTAA(uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    // Fetch the current pixel coordinate
    uint2 centerCoord = groupId * RAYTRACING_REFLECTION_TILE_SIZE + groupThreadId;
    centerCoord.x = centerCoord.x + (unity_StereoEyeIndex * _ScreenSize.x);

    float depth = LOAD_TEXTURE2D(_DepthTexture, centerCoord).r;
    PositionInputs posInputs = GetPositionInput_Stereo(centerCoord, _ScreenSize.zw, depth, UNITY_MATRIX_I_VP, UNITY_MATRIX_V, unity_StereoEyeIndex);

    float2 closest = GetClosestFragment(posInputs);

    float2 velocity;
    DecodeVelocity(LOAD_TEXTURE2D_X(_CameraMotionVectorsTexture, closest), velocity);
    float velocityLength = length(velocity);

    float2 uv = posInputs.positionNDC;

    float3 color = Fetch(_DenoiseInputTexture, uv, 0.0, _ScreenToTargetScale.xy);
    float3 history = Fetch(_ReflectionHistorybufferRW, posInputs.positionNDC - velocity, 0.0, _ScreenToTargetScaleHistory.xy);
    float3 topLeft = Fetch(_DenoiseInputTexture, uv, -RADIUS, _ScreenToTargetScale.xy);
    float3 bottomRight = Fetch(_DenoiseInputTexture, uv, RADIUS, _ScreenToTargetScale.xy);

    float3 corners = 4.0 * (topLeft + bottomRight) - 2.0 * color;

    color = clamp(color, 0.0, CLAMP_MAX);

    float3 average = Map((corners + color) / 7.0);

    topLeft = Map(topLeft);
    bottomRight = Map(bottomRight);
    color = Map(color);

    float colorLuma = Luminance(color);
    float averageLuma = Luminance(average);
    float nudge = lerp(4.0, 0.25, saturate(velocityLength * 100.0)) * abs(averageLuma - colorLuma);

    float3 minimum = min(bottomRight, topLeft) - nudge;
    float3 maximum = max(topLeft, bottomRight) + nudge;

    history = Map(history);

    // Clip history samples
    history = ClipToAABB(history, minimum, maximum);

    // Blend color & history
    // Feedback weight from unbiased luminance diff (Timothy Lottes)
    float historyLuma = Luminance(history);
    float diff = abs(colorLuma - historyLuma) / Max3(colorLuma, historyLuma, 0.2);
    float weight = 1.0 - diff;
    float feedback = lerp(FEEDBACK_MIN, FEEDBACK_MAX, weight * weight);

    color = Unmap(lerp(color, history, feedback));
    color = clamp(color, 0.0, CLAMP_MAX);
    
    _DenoiseOutputTextureRW[centerCoord] = float4(color, _DenoiseInputTexture[centerCoord].w);
}

// ----------------------------------------------------------------------------
// Denoising Kernel
// ----------------------------------------------------------------------------

// Couple helper functions
float sqr(float value)
{
    return value * value;
}
float gaussian(float radius, float sigma)
{
    return exp(-sqr(radius / sigma));
}

// Bilateral filter parameters
#define NORMAL_WEIGHT   1.0
#define PLANE_WEIGHT    1.0
#define DEPTH_WEIGHT    1.0

struct BilateralData
{
    float3 position;
    float3 normal;
    float perceptualRoughness;
    float  z;
};

BilateralData TapBilateralData(uint2 coordSS)
{
    BilateralData key;
    PositionInputs posInput;

    if (DEPTH_WEIGHT > 0.0 || PLANE_WEIGHT > 0.0)
    {
        posInput.deviceDepth = LOAD_TEXTURE2D(_DepthTexture, coordSS).r;
        key.z = Linear01Depth(posInput.deviceDepth, _ZBufferParams);
    }

    if (PLANE_WEIGHT > 0.0)
    {
        posInput = GetPositionInput_Stereo(coordSS, _ScreenSize.zw, posInput.deviceDepth,
                                           UNITY_MATRIX_I_VP, UNITY_MATRIX_V, unity_StereoEyeIndex);
        key.position = posInput.positionWS;
    }

    if ((NORMAL_WEIGHT > 0.0) || (PLANE_WEIGHT > 0.0))
    {
        NormalData normalData;
        const float4 normalBuffer = _NormalBufferTexture[COORD_TEXTURE2D_X(coordSS)];
        DecodeFromNormalBuffer(normalBuffer, coordSS, normalData);
        key.normal = normalData.normalWS;
        key.perceptualRoughness = normalData.perceptualRoughness;
    }

    return key;
}

float ComputeBilateralWeight(BilateralData center, BilateralData tap)
{
    float depthWeight    = 1.0;
    float normalWeight   = 1.0;
    float planeWeight    = 1.0;

    if (DEPTH_WEIGHT > 0.0)
    {
        depthWeight = max(0.0, 1.0 - abs(tap.z - center.z) * DEPTH_WEIGHT);
    }

    if (NORMAL_WEIGHT > 0.0)
    {
        const float normalCloseness = sqr(sqr(max(0.0, dot(tap.normal, center.normal))));
        const float normalError = 1.0 - normalCloseness;
        normalWeight = max(0.0, (1.0 - normalError * NORMAL_WEIGHT));
    }

    if (PLANE_WEIGHT > 0.0)
    {
        // Change in position in camera space
        const float3 dq = center.position - tap.position;

        // How far away is this point from the original sample
        // in camera space? (Max value is unbounded)
        const float distance2 = dot(dq, dq);

        // How far off the expected plane (on the perpendicular) is this point? Max value is unbounded.
        const float planeError = max(abs(dot(dq, tap.normal)), abs(dot(dq, center.normal)));

        planeWeight = (distance2 < 0.0001) ? 1.0 :
            pow(max(0.0, 1.0 - 2.0 * PLANE_WEIGHT * planeError / sqrt(distance2)), 2.0);
    }

    return depthWeight * normalWeight * planeWeight;
}

// Separated bilateral filter (two passes, each with 2*Radius taps)
[numthreads(RAYTRACING_REFLECTION_TILE_SIZE, RAYTRACING_REFLECTION_TILE_SIZE, 1)]
void ReflBilateralFilter(uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    // Fetch the current pixel coordinate
    uint2 centerCoord = groupId * RAYTRACING_REFLECTION_TILE_SIZE + groupThreadId;
    centerCoord.x = centerCoord.x + (unity_StereoEyeIndex * _ScreenSize.x);

    float3 reflSum = 0.0;
    float wSum = 0.0;

    #if FINAL_PASS
    const uint2 passIncr = uint2(1, 0);
    #else
    const uint2 passIncr = uint2(0, 1);
    #endif

    // Read the data of the center pixel
    const BilateralData center = TapBilateralData(centerCoord);

    // In order to avoid over-blurring, we define the size of the kernel based on the roughness of the surface
    const float realRadius = max(1, _RaytracingDenoiseRadius * sqrt(center.perceptualRoughness));
    const float sigma = 0.5 * realRadius;
    const int effectiveRadius = min(sigma * 2.0, realRadius);

    uint2 tapCoord = centerCoord - effectiveRadius * passIncr;
    for (int r = -effectiveRadius; r <= effectiveRadius; ++r, tapCoord += passIncr)
    {
        // Compute the weight (skip computation for the center)
        const float w = r ? gaussian(r, sigma) * ComputeBilateralWeight(center, TapBilateralData(tapCoord)) : 1.0;

        reflSum += _DenoiseInputTexture[tapCoord].xyz * w;
        wSum += w;
    }

    // Store the intermediate result
    float3 reflection = reflSum / wSum;
    _DenoiseOutputTextureRW[centerCoord] = float4(reflection, 1.0);
}
